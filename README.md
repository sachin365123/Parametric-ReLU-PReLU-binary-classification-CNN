# Parametric-ReLU-PReLU-binary-classification-CNN
ReLU (Rectified Linear Unit) is a popular activation function used in deep neural networks. However, there are several other activation functions that can be used instead of ReLU, depending on the specific requirements of your model and data. Here are some alternatives
